import 'dart:io';
import 'package:record/record.dart';
import 'package:path_provider/path_provider.dart';
import 'package:ffmpeg_kit_flutter/ffmpeg_kit.dart';
import 'package:ffmpeg_kit_flutter/return_code.dart';
import 'package:collection/collection.dart';
import '../data/models/record/tts_segment_model.dart';
import '../core/utils/logs/audio_logger.dart';
import 'dart:async';
import 'package:dio/dio.dart';
import 'package:path/path.dart' as p;
import 'package:http_parser/http_parser.dart';
import '../../../../../core/constants/api/summary_api.dart';

class CallRecordingService {
  final _recorder = AudioRecorder();
  String? _currentFilePath;

  Future<void> startRecording() async {
    final hasPermission = await _recorder.hasPermission();
    if (!hasPermission) {
      throw Exception('ğŸ™ï¸ Microphone permission denied');
    }

    final dir = await getApplicationDocumentsDirectory();
    final filePath =
        '${dir.path}/conversation_${DateTime.now().millisecondsSinceEpoch}.wav';
    _currentFilePath = filePath;

    final config = RecordConfig(
      encoder: AudioEncoder.wav,
      bitRate: 128000,
      sampleRate: 44100,
      numChannels: 1,
      echoCancel: true,
    );

    await _recorder.start(
      config,
      path: filePath,
    );
  }

  Future<String?> stopRecording() async {
    if (await _recorder.isRecording()) {
      await _recorder.stop();
      await AudioLogger.printWavInfo(_currentFilePath!);

      return _currentFilePath;
    }
    return null;
  }

  Future<bool> isRecording() async => await _recorder.isRecording();

  Future<String?> mergeAudioFiles({
    required String micPath,
    required List<TtsSegmentModel> ttsSegments,
    required String outputFileName,
    required int durationMs,
  }) async {
    final outputDir = (await getApplicationDocumentsDirectory()).path;
    final outputPath = '$outputDir/$outputFileName.wav';
    if (ttsSegments.isEmpty) {
      print("âš ï¸ TTS ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ. ì›ë³¸ íŒŒì¼ë§Œ ë³µì‚¬");
      final originalFile = File(micPath);
      await originalFile.copy(outputPath);
      AudioLogger.printWavInfo(outputPath);
      _sendMergedAudioAndPrintSummary(outputPath);
      return outputPath;
    }


    AudioLogger.printWavInfo(micPath);
    // ì…ë ¥ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    final inputs = <String>["-i '$micPath'"];
    for (final seg in ttsSegments) {
      inputs.add("-i '${seg.audioPath}'");
      AudioLogger.printWavInfo(seg.audioPath);
    }

    // ffmpeg ì‹¤í–‰
    final durationSec = (durationMs / 1000).toStringAsFixed(2);
    final filterComplex = [
      "[0:0]atrim=duration=${durationSec}[mic]",
      ...ttsSegments.mapIndexed(
          (i, seg) => "[${i + 1}:0]adelay=${seg.startMs}|${seg.startMs}[td$i]"),
      "[mic]" +
          List.generate(ttsSegments.length, (i) => "[td$i]").join() +
          "amix=inputs=${ttsSegments.length + 1}:duration=first:dropout_transition=0[aout]"
    ].join("; ");

    final command = [
      ...inputs,
      "-t $durationSec",
      "-filter_complex \"$filterComplex\"",
      "-map \"[aout]\"",
      "-c:a pcm_s16le -ar 44100 -ac 1",
      "'$outputPath'"
    ].join(' ');

    print("ğŸ›ï¸ FFmpeg ëª…ë ¹ì–´:\n$command");

    final session = await FFmpegKit.execute(command);

    final returnCode = await session.getReturnCode();
    if (ReturnCode.isSuccess(returnCode)) {
      print("âœ… ë¯¹ì‹± ì™„ë£Œ: $outputPath");
      AudioLogger.printWavInfo(outputPath);
      _sendMergedAudioAndPrintSummary(outputPath);
      return outputPath;
    } else {
      print("âŒ ë¯¹ì‹± ì‹¤íŒ¨: ${returnCode?.getValue()}");
      return null;
    }
  }

  Future<void> _sendMergedAudioAndPrintSummary(String filePath) async {
    final dio = Dio();
    final serverUrl = SummaryApi.summary;

    try {
      final fileName = p.basename(filePath);
      final formData = FormData.fromMap({
        'file': await MultipartFile.fromFile(
          filePath,
          filename: fileName,
          contentType: MediaType('audio', 'wav'),
        ),
      });

      final response = await dio.post(serverUrl, data: formData);

      if (response.statusCode == 200) {
        final data = response.data;
        print('ğŸ“„ í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼: ${data['transcription']}');
        print('ğŸ§  ìš”ì•½ ì‘ë‹µ: ${data['gpt_response']}');
      } else {
        print('âŒ ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${response.statusCode}');
      }
    } catch (e) {
      print('ğŸš¨ ì „ì†¡ ì‹¤íŒ¨: $e');
    }
  }

  Future<void> deleteAudioResources({
    required String micPath,
    required List<String> ttsPaths,
  }) async {
    try {
      // ğŸ¤ ë§ˆì´í¬ ë…¹ìŒ íŒŒì¼ ì‚­ì œ
      final micFile = File(micPath);
      if (await micFile.exists()) {
        await micFile.delete();
        print('ğŸ—‘ï¸ ë§ˆì´í¬ íŒŒì¼ ì‚­ì œë¨: $micPath');
      }

      // ğŸ§  TTS íŒŒì¼ë“¤ ì‚­ì œ
      for (final path in ttsPaths) {
        final ttsFile = File(path);
        if (await ttsFile.exists()) {
          await ttsFile.delete();
          print('ğŸ—‘ï¸ TTS íŒŒì¼ ì‚­ì œë¨: $path');
        }
      }
    } catch (e) {
      print('âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: $e');
    }
  }
}
